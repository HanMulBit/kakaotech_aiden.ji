{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cdb40bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\ys\\anaconda3\\lib\\site-packages (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\ys\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.3.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\ys\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\ys\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\ys\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\ys\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\ys\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\ys\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef596ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP 1: 객체 간 관계성 인지용 한국형 비전 데이터\n",
      "TOP 2: 손∙팔 협조에 의한 파지-조작 동작 데이터\n",
      "TOP 3: 상용 자율주행차 야간 자동차 전용도로 데이터\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# AI Hub 페이지 URL\n",
    "url = 'https://www.aihub.or.kr/'\n",
    "\n",
    "# 웹 페이지 요청\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()  # 요청이 성공했는지 확인\n",
    "\n",
    "# BeautifulSoup 객체 생성\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# 인기 데이터 TOP3 섹션 찾기\n",
    "top3_section = soup.find('div', class_='secR')\n",
    "\n",
    "# 각 데이터 항목 추출\n",
    "data_list = top3_section.find_all('div', class_='list')\n",
    "\n",
    "# 데이터 제목 추출\n",
    "titles = []\n",
    "for data in data_list:\n",
    "    title = data.find('h3').get_text(strip=True)\n",
    "    clean_title = title.split(']')[-1].strip()\n",
    "    titles.append(clean_title)\n",
    "\n",
    "# 추출한 데이터 출력\n",
    "for idx, title in enumerate(titles, start=1):\n",
    "    print(f\"TOP {idx}: {title}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "974ac5d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: 위키백과:대문\n",
      "First paragraph:  위키백과\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 1. 웹 페이지 요청\n",
    "url = 'https://ko.wikipedia.org/wiki/위키백과:대문'\n",
    "response = requests.get(url)\n",
    "\n",
    "# 2. 요청이 성공했는지 확인\n",
    "if response.status_code == 200:\n",
    "    # 3. BeautifulSoup 객체 생성\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # 4. 페이지 제목 추출\n",
    "    title = soup.find('h1', id='firstHeading').text\n",
    "    print(f\"Title: {title}\")\n",
    "\n",
    "    # 5. 첫 번째 단락 추출\n",
    "    first_paragraph = soup.find('p').text\n",
    "    print(f\"First paragraph: {first_paragraph}\")\n",
    "else:\n",
    "    print(f\"Failed to retrieve the web page. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "849ec512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Downloading selenium-4.22.0-py3-none-any.whl (9.4 MB)\n",
      "     ---------------------------------------- 9.4/9.4 MB 10.1 MB/s eta 0:00:00\n",
      "Collecting trio-websocket~=0.9\n",
      "  Downloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
      "Collecting typing_extensions>=4.9.0\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Collecting websocket-client>=1.8.0\n",
      "  Downloading websocket_client-1.8.0-py3-none-any.whl (58 kB)\n",
      "     ---------------------------------------- 58.8/58.8 kB ? eta 0:00:00\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\ys\\appdata\\roaming\\python\\python39\\site-packages (from selenium) (2023.7.22)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in c:\\users\\ys\\anaconda3\\lib\\site-packages (from selenium) (1.26.11)\n",
      "Collecting trio~=0.17\n",
      "  Downloading trio-0.25.1-py3-none-any.whl (467 kB)\n",
      "     -------------------------------------- 467.7/467.7 kB 9.7 MB/s eta 0:00:00\n",
      "Collecting sniffio>=1.3.0\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\ys\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Collecting attrs>=23.2.0\n",
      "  Using cached attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "Collecting exceptiongroup\n",
      "  Downloading exceptiongroup-1.2.1-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\ys\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
      "Requirement already satisfied: idna in c:\\users\\ys\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (2.10)\n",
      "Collecting outcome\n",
      "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting wsproto>=0.14\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\ys\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\ys\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Collecting h11<1,>=0.9.0\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "     ---------------------------------------- 58.3/58.3 kB ? eta 0:00:00\n",
      "Installing collected packages: websocket-client, typing_extensions, sniffio, h11, exceptiongroup, attrs, wsproto, outcome, trio, trio-websocket, selenium\n",
      "  Attempting uninstall: websocket-client\n",
      "    Found existing installation: websocket-client 0.58.0\n",
      "    Uninstalling websocket-client-0.58.0:\n",
      "      Successfully uninstalled websocket-client-0.58.0\n",
      "  Attempting uninstall: typing_extensions\n",
      "    Found existing installation: typing_extensions 4.3.0\n",
      "    Uninstalling typing_extensions-4.3.0:\n",
      "      Successfully uninstalled typing_extensions-4.3.0\n",
      "  Attempting uninstall: sniffio\n",
      "    Found existing installation: sniffio 1.2.0\n",
      "    Uninstalling sniffio-1.2.0:\n",
      "      Successfully uninstalled sniffio-1.2.0\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 21.4.0\n",
      "    Uninstalling attrs-21.4.0:\n",
      "      Successfully uninstalled attrs-21.4.0\n",
      "Successfully installed attrs-23.2.0 exceptiongroup-1.2.1 h11-0.14.0 outcome-1.3.0.post0 selenium-4.22.0 sniffio-1.3.1 trio-0.25.1 trio-websocket-0.11.1 typing_extensions-4.12.2 websocket-client-1.8.0 wsproto-1.2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\ys\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\ys\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\ys\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -illow (c:\\users\\ys\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -illow (c:\\users\\ys\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -illow (c:\\users\\ys\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -illow (c:\\users\\ys\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\ys\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\ys\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\ys\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\ys\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\ys\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\ys\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\ys\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\ys\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\ys\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\ys\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\ys\\anaconda3\\lib\\site-packages)\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "black 22.6.0 requires click>=8.0.0, but you have click 7.1.2 which is incompatible.\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\ys\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\ys\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\ys\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53a95a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Content: 우리 모두가 만들어가는 자유 백과사전\n",
      "문서 674,592개와 최근 기여자 1,814명\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# URL 설정\n",
    "URL = \"https://ko.wikipedia.org/wiki/위키백과:대문\"\n",
    "\n",
    "# Chrome 옵션 설정\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--headless\") # 브라우저 창을 띄우지 않음\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "\n",
    "# 웹 드라이버 설정\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "try:\n",
    "    # 위키백과 대문 페이지 열기\n",
    "    driver.get(URL)\n",
    "\n",
    "    # \"우리 모두가 만들어가는 자유 백과사전\"과 \"문서 이하 내용\" 추출\n",
    "    main_content = driver.find_element(By.CSS_SELECTOR, \"#mw-content-text > div.mw-content-ltr.mw-parser-output > div.main-box.main-top > div > p:nth-child(2)\").text\n",
    "    print(\"Main Content:\", main_content)\n",
    "finally:\n",
    "    # 웹 드라이버 종료\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d0f8749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scrapy in c:\\users\\ys\\anaconda3\\lib\\site-packages (2.6.2)\n",
      "Requirement already satisfied: cssselect>=0.9.1 in c:\\users\\ys\\anaconda3\\lib\\site-packages (from scrapy) (1.1.0)\n",
      "Requirement already satisfied: itemadapter>=0.1.0 in c:\\users\\ys\\anaconda3\\lib\\site-packages (from scrapy) (0.3.0)\n",
      "Requirement already satisfied: cryptography>=2.0 in c:\\users\\ys\\anaconda3\\lib\\site-packages (from scrapy) (37.0.1)\n",
      "Requirement already satisfied: protego>=0.1.15 in c:\\users\\ys\\anaconda3\\lib\\site-packages (from scrapy) (0.1.16)\n",
      "Requirement already satisfied: PyDispatcher>=2.0.5 in c:\\users\\ys\\anaconda3\\lib\\site-packages (from scrapy) (2.0.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ys\\anaconda3\\lib\\site-packages (from scrapy) (63.4.1)\n",
      "Requirement already satisfied: pyOpenSSL>=16.2.0 in c:\\users\\ys\\anaconda3\\lib\\site-packages (from scrapy) (22.0.0)\n",
      "Requirement already satisfied: parsel>=1.5.0 in c:\\users\\ys\\anaconda3\\lib\\site-packages (from scrapy) (1.6.0)\n",
      "Requirement already satisfied: Twisted>=17.9.0 in c:\\users\\ys\\anaconda3\\lib\\site-packages (from scrapy) (22.2.0)\n",
      "Requirement already satisfied: service-identity>=16.0.0 in c:\\users\\ys\\anaconda3\\lib\\site-packages (from scrapy) (18.1.0)\n",
      "Requirement already satisfied: zope.interface>=4.1.3 in c:\\users\\ys\\anaconda3\\lib\\site-packages (from scrapy) (5.4.0)\n",
      "Requirement already satisfied: lxml>=3.5.0 in c:\\users\\ys\\anaconda3\\lib\\site-packages (from scrapy) (4.9.1)\n",
      "Requirement already satisfied: queuelib>=1.4.2 in c:\\users\\ys\\anaconda3\\lib\\site-packages (from scrapy) (1.5.0)\n",
      "Requirement already satisfied: w3lib>=1.17.0 in c:\\users\\ys\\anaconda3\\lib\\site-packages (from scrapy) (1.21.0)\n",
      "Requirement already satisfied: tldextract in c:\\users\\ys\\anaconda3\\lib\\site-packages (from scrapy) (3.2.0)\n",
      "Requirement already satisfied: itemloaders>=1.0.1 in c:\\users\\ys\\anaconda3\\lib\\site-packages (from scrapy) (1.0.4)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\ys\\anaconda3\\lib\\site-packages (from cryptography>=2.0->scrapy) (1.15.1)\n",
      "Requirement already satisfied: jmespath>=0.9.5 in c:\\users\\ys\\anaconda3\\lib\\site-packages (from itemloaders>=1.0.1->scrapy) (0.10.0)\n",
      "Requirement already satisfied: six>=1.6.0 in c:\\users\\ys\\anaconda3\\lib\\site-packages (from parsel>=1.5.0->scrapy) (1.16.0)\n",
      "Requirement already satisfied: pyasn1 in c:\\users\\ys\\anaconda3\\lib\\site-packages (from service-identity>=16.0.0->scrapy) (0.4.8)\n",
      "Requirement already satisfied: pyasn1-modules in c:\\users\\ys\\anaconda3\\lib\\site-packages (from service-identity>=16.0.0->scrapy) (0.2.8)\n",
      "Requirement already satisfied: attrs>=16.0.0 in c:\\users\\ys\\anaconda3\\lib\\site-packages (from service-identity>=16.0.0->scrapy) (23.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in c:\\users\\ys\\anaconda3\\lib\\site-packages (from Twisted>=17.9.0->scrapy) (4.12.2)\n",
      "Requirement already satisfied: incremental>=21.3.0 in c:\\users\\ys\\anaconda3\\lib\\site-packages (from Twisted>=17.9.0->scrapy) (21.3.0)\n",
      "Requirement already satisfied: Automat>=0.8.0 in c:\\users\\ys\\anaconda3\\lib\\site-packages (from Twisted>=17.9.0->scrapy) (20.2.0)\n",
      "Requirement already satisfied: constantly>=15.1 in c:\\users\\ys\\anaconda3\\lib\\site-packages (from Twisted>=17.9.0->scrapy) (15.1.0)\n",
      "Requirement already satisfied: hyperlink>=17.1.1 in c:\\users\\ys\\anaconda3\\lib\\site-packages (from Twisted>=17.9.0->scrapy) (21.0.0)\n",
      "Requirement already satisfied: twisted-iocpsupport<2,>=1.0.2 in c:\\users\\ys\\anaconda3\\lib\\site-packages (from Twisted>=17.9.0->scrapy) (1.0.2)\n",
      "Requirement already satisfied: requests>=2.1.0 in c:\\users\\ys\\anaconda3\\lib\\site-packages (from tldextract->scrapy) (2.28.1)\n",
      "Requirement already satisfied: filelock>=3.0.8 in c:\\users\\ys\\anaconda3\\lib\\site-packages (from tldextract->scrapy) (3.6.0)\n",
      "Requirement already satisfied: idna in c:\\users\\ys\\appdata\\roaming\\python\\python39\\site-packages (from tldextract->scrapy) (2.10)\n",
      "Requirement already satisfied: requests-file>=1.4 in c:\\users\\ys\\anaconda3\\lib\\site-packages (from tldextract->scrapy) (1.5.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\ys\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography>=2.0->scrapy) (2.21)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\ys\\anaconda3\\lib\\site-packages (from requests>=2.1.0->tldextract->scrapy) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\ys\\anaconda3\\lib\\site-packages (from requests>=2.1.0->tldextract->scrapy) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ys\\appdata\\roaming\\python\\python39\\site-packages (from requests>=2.1.0->tldextract->scrapy) (2023.7.22)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\ys\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\ys\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\ys\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\ys\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\ys\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\ys\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "# @title\n",
    "!pip install scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8fc2816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Scrapy project 'wikipedia_scraper', using template directory 'C:\\Users\\Ys\\anaconda3\\lib\\site-packages\\scrapy\\templates\\project', created in:\n",
      "    C:\\Users\\Ys\\workspace\\python_kakao\\kakaotech_aiden.ji\\wikipedia_scraper\n",
      "\n",
      "You can start your first spider with:\n",
      "    cd wikipedia_scraper\n",
      "    scrapy genspider example example.com\n"
     ]
    }
   ],
   "source": [
    "# @title\n",
    "!scrapy startproject wikipedia_scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d4f838d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title\n",
    "# wikipedia_scraper/spiders/wikipedia_spider.py\n",
    "import scrapy\n",
    "\n",
    "class WikipediaSpider(scrapy.Spider):\n",
    "    name = \"wikipedia\"\n",
    "    start_urls = [\n",
    "        'https://ko.wikipedia.org/wiki/위키백과:대문',\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        main_content = response.css('#mw-content-text > div.mw-content-ltr.mw-parser-output > div.main-pane > div.main-pane-right > div.wikipedia-ko.main-recommended.main-box').get()\n",
    "        yield {\n",
    "            'main_content': main_content,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b63959eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to wikipedia_scraper/wikipedia_scraper/settings.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a wikipedia_scraper/wikipedia_scraper/settings.py\n",
    "ROBOTSTXT_OBEY = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de68cce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting run_scrapy.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile run_scrapy.py\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from wikipedia_scraper.spiders.wikipedia_spider import WikipediaSpider\n",
    "\n",
    "process = CrawlerProcess({\n",
    "    'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3',\n",
    "    'FEEDS': {\n",
    "        'output.json': {\n",
    "            'format': 'json',\n",
    "            'encoding': 'utf8',\n",
    "            'store_empty': False,\n",
    "            'fields': None,\n",
    "            'indent': 4,\n",
    "        },\n",
    "    },\n",
    "})\n",
    "process.crawl(WikipediaSpider)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dccef0cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Ys\\workspace\\python_kakao\\kakaotech_aiden.ji\\day1\\run_scrapy.py\", line 2, in <module>\n",
      "    from wikipedia_scraper.spiders.wikipedia_spider import WikipediaSpider\n",
      "ModuleNotFoundError: No module named 'wikipedia_scraper.spiders'\n"
     ]
    }
   ],
   "source": [
    "!python run_scrapy.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47054007",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'output.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8188\\4286355602.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpprint\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpprint\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'output.json'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mpprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'output.json'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "with open('output.json','r') as f:\n",
    "    data = json.load(f)\n",
    "    pprint(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c41046",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
